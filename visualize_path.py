import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.patches import FancyArrowPatch
from mpl_toolkits.mplot3d import proj3d
import re

#This code reads results from the results file generated by unik3d, mast3r and madpose trio for pose estimation using monocular priors.
#Results are in the format of:
#From 0.000000 to 0.000000
#Estimated rotation:
#0.999999 0.000000 0.000000
#0.000000 0.999999 0.000000
#0.000000 0.000000 0.999999
#Estimated translation:
#0.000000 0.000000 0.000000
#GT rotation:
#0.999999 0.000000 0.000000
#0.000000 0.999999 0.000000
#0.000000 0.000000 0.999999
#GT translation:
#0.000000 0.000000 0.000000
#Number of inliers: 1000
#Number of matches: 1000
#==============================


class Arrow3D(FancyArrowPatch):
    def __init__(self, xs, ys, zs, *args, **kwargs):
        super().__init__((0,0), (0,0), *args, **kwargs)
        self._verts3d = xs, ys, zs

    def do_3d_projection(self, renderer=None):
        xs3d, ys3d, zs3d = self._verts3d
        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)
        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))
        return np.min(zs)

def procrustes_align(X, Y):
    """
    Align X to Y using Procrustes analysis
    X: estimated path (N x 3)
    Y: ground truth path (N x 3)
    Returns: aligned X, scale, rotation, translation
    """
    
    # Center the data
    X_centered = X - np.mean(X, axis=0)
    Y_centered = Y - np.mean(Y, axis=0)
    
    # Scale to unit size
    X_scale = np.sqrt(np.sum(X_centered ** 2))
    Y_scale = np.sqrt(np.sum(Y_centered ** 2))
    X_normalized = X_centered / X_scale
    Y_normalized = Y_centered / Y_scale
    
    # Compute rotation
    H = X_normalized.T @ Y_normalized
    U, _, Vt = np.linalg.svd(H)
    R = Vt.T @ U.T
    
    # Handle reflection case
    if np.linalg.det(R) < 0:
        Vt[-1, :] *= -1
        R = Vt.T @ U.T
    
    # Compute scale
    scale = Y_scale / X_scale
    
    # Compute translation
    t = np.mean(Y, axis=0) - scale * (R @ np.mean(X, axis=0))
    
    # Apply transformation
    X_aligned = scale * (X @ R.T) + t
    #X_aligned = (X @ R.T) + t
    return X_aligned, scale, R, t

def extract_matrix(text):
    # Find all numbers in the text, including scientific notation
    numbers = re.findall(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", text)
    if len(numbers) >= 9:  # We need 9 numbers for a 3x3 matrix
        matrix = np.array([float(x) for x in numbers[:9]]).reshape(3, 3)
        return matrix
    return None

def extract_translation(text):
    # Find all numbers in the text, including scientific notation
    numbers = re.findall(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", text)
    if len(numbers) >= 3:  # We need 3 numbers for translation
        return np.array([float(x) for x in numbers[:3]])
    return None

def read_poses(filename):
    poses = []
    
    with open(filename, 'r') as f:
        content = f.read()
        
    # Split content into pose blocks
    blocks = content.split('==============================')
    
    for block in blocks:
        if not block.strip():
            continue
            
        pose = {}
        
        # Extract time
        time_match = re.search(r'From (\d+\.\d+) to', block)
        if time_match:
            pose['time'] = float(time_match.group(1))
        
        # Extract estimated rotation
        est_rot_match = re.search(r'Estimated rotation:(.*?)(?=GT rotation:|$)', block, re.DOTALL)
        if est_rot_match:
            pose['est_rot'] = extract_matrix(est_rot_match.group(1))
        
        # Extract GT rotation
        gt_rot_match = re.search(r'GT rotation:(.*?)(?=Estimated translation:|$)', block, re.DOTALL)
        if gt_rot_match:
            pose['gt_rot'] = extract_matrix(gt_rot_match.group(1))
        
        # Extract estimated translation
        est_trans_match = re.search(r'Estimated translation:(.*?)(?=GT translation:|$)', block, re.DOTALL)
        if est_trans_match:
            pose['est_trans'] = extract_translation(est_trans_match.group(1))
        
        # Extract GT translation
        gt_trans_match = re.search(r'GT translation:(.*?)(?=From|$)', block, re.DOTALL)
        if gt_trans_match:
            pose['gt_trans'] = extract_translation(gt_trans_match.group(1))
        
        inlier_match = re.search(r'Number of inliers: (\d+)', block)
        if inlier_match:
            pose['inliers'] = int(inlier_match.group(1))

        match = re.search(r'Number of matches: (\d+)', block)
        if match:
            pose['matches'] = int(match.group(1))
        # Only add pose if it has all required components
        if all(key in pose for key in ['time', 'est_rot', 'gt_rot', 'est_trans', 'gt_trans', 'inliers', 'matches']):
            poses.append(pose)
    
    return poses

def plot_pose(ax, position, rotation, scale=0.1, color='r'):
    # Plot coordinate frame
    for i in range(3):
        direction = rotation[:, i] * scale
        arrow = Arrow3D([position[0], position[0] + direction[0]],
                       [position[1], position[1] + direction[1]],
                       [position[2], position[2] + direction[2]],
                       mutation_scale=10, lw=1, arrowstyle='-|>', color=color)
        ax.add_artist(arrow)



def draw_camera_frustum(ax, R, t, scale=0.1, color='b', label=None):
    """
    R: 3x3 rotation matrix (global orientation)
    t: 3-vector translation (global position)
    scale: frustum büyüklüğü
    """
    # Kamera merkez noktası
    cam_center = t

    # Kamera yerel koordinat sisteminde frustum köşe noktaları (piramid)
    # Near plane (kamera önünde)
    near = scale
    width = scale * 0.7
    height = scale * 0.5
    
    # Yerel koordinatlarda 5 nokta (merkez + 4 köşe)
    points = np.array([
        [0, 0, 0],                      # Kamera merkezi
        [ width,  height, near],        # Üst sağ ön
        [ width, -height, near],        # Alt sağ ön
        [-width, -height, near],        # Alt sol ön
        [-width,  height, near],        # Üst sol ön
    ]).T  # 3 x 5
    
    # Global koordinatlara dönüştür (R ve t kullanarak)
    points_global = (R @ points) + t.reshape(3,1)  # 3 x 5

    # Kamera merkezini ve ön planı çiz
    # Merkezden ön planın 4 köşesine çizgiler
    for i in range(1,5):
        xs = [points_global[0,0], points_global[0,i]]
        ys = [points_global[1,0], points_global[1,i]]
        zs = [points_global[2,0], points_global[2,i]]
        ax.plot(xs, ys, zs, color=color)
    
    # Ön planı kapatmak için köşeleri birleştir
    corners = [1,2,3,4,1]
    for i in range(4):
        xs = [points_global[0,corners[i]], points_global[0,corners[i+1]]]
        ys = [points_global[1,corners[i]], points_global[1,corners[i+1]]]
        zs = [points_global[2,corners[i]], points_global[2,corners[i+1]]]
        ax.plot(xs, ys, zs, color=color)
    
    # Kamerayı küçük bir nokta ile göster
    ax.scatter(t[0], t[1], t[2], color=color, marker='o', s=20)
    
    if label is not None:
        ax.text(t[0], t[1], t[2], label, color=color)

def main():
    # Read poses
    poses = read_poses('200ms_reverse_pg.txt')
    print(f"Found {len(poses)} complete poses")
    #take first 30 poses
   
    #read gyro data from data_swapped_no_bias.csv
    gyro_data = np.genfromtxt("data_swapped_no_bias.csv", delimiter=",")
    gyro_data = gyro_data[0:]
    print(gyro_data.shape)

        

    # Create figure with two subplots
    fig = plt.figure(figsize=(20, 8))
    ax_3d = fig.add_subplot(121, projection='3d')
    ax_2d = fig.add_subplot(122)
    
    # Initialize positions and rotations
    est_pos = np.zeros(3)
    gt_pos = np.zeros(3)
    est_rot = np.eye(3)  # Initial rotation is identity
    gt_rot = np.eye(3)   # Initial rotation is identity
    
    # Plot initial pose
    plot_pose(ax_3d, est_pos, est_rot, color='r')
    plot_pose(ax_3d, gt_pos, gt_rot, color='b')
    
    # Arrays to store positions and rotations for plotting
    est_positions = [est_pos.copy()]
    gt_positions = [gt_pos.copy()]
    est_rotations = [est_rot.copy()]
    gt_rotations = [gt_rot.copy()]
    
    # Process each pose
    for pose in poses:
        gyro_data_current = gyro_data[int(pose['time']*400-80):int(pose['time']*400)]
        
        # Initialize rotation matrix as identity
        R = np.eye(3)
        
        # For each IMU measurement (2.5ms step)
        for w in gyro_data_current:
            # Convert angular velocity to rotation matrix for small time step
            # Using first order approximation: R = I + [w]x * dt
            dt = 0.0025  # 2.5ms
            
            # Check for NaN or inf values in angular velocity
            if np.any(np.isnan(w)) or np.any(np.isinf(w)):
                print(f"Warning: Invalid angular velocity values detected: {w}")
                continue
                
            w_skew = np.array([[0, -w[3], w[2]],
                              [w[3], 0, -w[1]], 
                              [-w[2], w[1], 0]])
            
            # Add small regularization to prevent numerical instability
            dR = np.eye(3) + w_skew * dt
            
            # Ensure dR is orthogonal by normalizing
            u, s, vh = np.linalg.svd(dR)
            dR = u @ vh

            # Update total rotation and check for validity
            R = R @ dR
            
            if np.any(np.isnan(R)):
                print("Warning: NaN values detected in rotation matrix")
                print("Current angular velocity:", w)
                print("Current dR:", dR)
                R = np.eye(3)  # Reset to identity if invalid
                

        ## Update global position by applying local translation transformed by current rotation
        est_pos = est_pos + est_rot @ pose['est_trans']
        gt_pos = gt_pos + gt_rot @ pose['gt_trans']
        #u, s, vh = np.linalg.svd(pose['est_rot'])
        #print(np.linalg.svd(pose['est_rot']))
         #print(s)
        #print(ratio)
        temp_rot = (0.3 * pose['est_rot'] + 0.7 * R ) 
        #temp_rot = (pose['est_rot'] + 2.5 * R ) /3.5
        #temp_rot = R 
        # Update global rotation by composing with relative rotation
        est_rot = est_rot @ temp_rot
        gt_rot = gt_rot @ pose['gt_rot']
        
        # Store positions and rotations
        est_positions.append(est_pos.copy())
        gt_positions.append(gt_pos.copy())
        est_rotations.append(est_rot.copy())
        gt_rotations.append(gt_rot.copy())
        
        # Plot coordinate frames every 10 poses (or change frequency as needed)
        if len(est_positions) % 20 == 0:
            plot_pose(ax_3d, est_pos, est_rot, scale=0.05, color='r')
            plot_pose(ax_3d, gt_pos, gt_rot, scale=0.05, color='b')
            draw_camera_frustum(ax_3d, est_rot, est_pos, scale=0.5, color='g')
    
    # Convert to numpy arrays
    est_positions = np.array(est_positions)
    gt_positions = np.array(gt_positions)
    
    # Align estimated path to ground truth
    est_positions_aligned, scale, R, t = procrustes_align(est_positions, gt_positions)
    
    # Plot 3D paths
    ax_3d.plot(est_positions[:, 0], est_positions[:, 1], est_positions[:, 2], 'r-', label='Estimated Path')
    ax_3d.plot(gt_positions[:, 0], gt_positions[:, 1], gt_positions[:, 2], 'b-', label='Ground Truth Path')
    ax_3d.plot(est_positions_aligned[:, 0], est_positions_aligned[:, 1], est_positions_aligned[:, 2], 
               'g--', label='Aligned Estimated Path')
    
    # Plot 2D XZ projection
    #ax_2d.plot(est_positions[:, 0], est_positions[:, 2], 'r-', label='Estimated Path')
    ax_2d.plot(gt_positions[:, 0], gt_positions[:, 2], 'b-', label='Ground Truth Path')
    ax_2d.plot(est_positions_aligned[:, 0], est_positions_aligned[:, 2], 'r-', label='Aligned Estimated Path')
    
    # Print alignment statistics
    print(f"\nAlignment Statistics:")
    print(f"Scale factor: {scale:.6f}")
    print(f"Translation: {t}")
    print(f"Rotation matrix:\n{R}")
    
    # Calculate error metrics
    original_error = np.mean(np.linalg.norm(est_positions - gt_positions, axis=1))
    #take rmse of the aligned trajectory
    aligned_error = np.sqrt(np.mean(np.linalg.norm(est_positions_aligned - gt_positions, axis=1)**2))
    print(f"\nError Metrics:")
    print(f"Original mean error: {original_error:.6f}")
    print(f"Aligned mean error: {aligned_error:.6f}")
    
    ## Draw all camera coordinate frames (exes, eyes, zees) with smaller scale
    #for pos, rot in zip(est_positions, est_rotations):
    #    plot_pose(ax_3d, pos, rot, scale=0.05, color='r')
    #for pos, rot in zip(gt_positions, gt_rotations):
    #    plot_pose(ax_3d, pos, rot, scale=0.05, color='b')
    
    # Set labels and legend for 3D plot
    ax_3d.set_xlabel('X')
    ax_3d.set_ylabel('Y')
    ax_3d.set_zlabel('Z')
    ax_3d.legend()
    
    # Set labels and legend for 2D plot
    ax_2d.set_xlabel('X')
    ax_2d.set_ylabel('Z')
    ax_2d.legend()
    ax_2d.grid(True)
    ax_2d.set_title('XZ Projection')
    
    # Set equal aspect ratio for 3D plot
    max_range = np.array([est_positions.max(axis=0) - est_positions.min(axis=0),
                         gt_positions.max(axis=0) - gt_positions.min(axis=0)]).max() / 2.0
    mid_x = (est_positions[:, 0].max() + est_positions[:, 0].min()) * 0.5
    mid_y = (est_positions[:, 1].max() + est_positions[:, 1].min()) * 0.5
    mid_z = (est_positions[:, 2].max() + est_positions[:, 2].min()) * 0.5
    ax_3d.set_xlim(mid_x - max_range, mid_x + max_range)
    ax_3d.set_ylim(mid_y - max_range, mid_y + max_range)
    ax_3d.set_zlim(mid_z - max_range, mid_z + max_range)
    
    # Set equal aspect ratio for 2D plot
    ax_2d.set_aspect('equal')
    
    plt.suptitle(f'Monocular Translation + Gyro Rotation Estimated vs Ground Truth Path (with Alignment), RMSE: {aligned_error:.3f}')
    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main() 